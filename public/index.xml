<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Zhiya&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Zhiya&#39;s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en</language>
    <lastBuildDate>Fri, 16 May 2025 21:52:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unique Indexes: We Should Think Twice (Especially at Scale)</title>
      <link>http://localhost:1313/posts/think-twice-on-unique-key/</link>
      <pubDate>Fri, 16 May 2025 21:52:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/think-twice-on-unique-key/</guid>
      <description>&lt;p&gt;In “Big Tech” environments (you know, the kind with tons of users, massive datasets, and rapidly evolving requirements), relying on database &lt;code&gt;UNIQUE INDEX&lt;/code&gt; constraints to prevent duplicate data—unless it’s for something like financial reconciliation where every penny must be exact—honestly, might not be as effective as you think. Plus, the cost of maintaining them can be surprisingly high. A better approach is often to handle the bulk of deduplication logic at the application layer. If you can avoid using a database unique index, consider doing so, or at least think it through very carefully before implementing one.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
